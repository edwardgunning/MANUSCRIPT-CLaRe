\section{Additional Details on Squared Correlation Loss} \label{sec:squared-correlation}

\subsection{Complement of the Predicted Correlation Squared ($1-\rho^2$)}

Our loss measure is the complement of the squared correlation correlation among the observed data $X_i (\mathbf{t})$ and its predicted value $\widehat{X}^{(K)}_{i} (\mathbf{t})$:
$$
1- \rho^2 \left\{X_i (\mathbf{t}), \widehat{X}^{(K)}_{i} (\mathbf{t}) \right\} =
1 - \frac{\left[ \mathlarger{\sum}_{t = 1}^T{\bigg\{X_i (t) - \overline{X}_i \bigg\} \bigg\{ \widehat{X}_i^{(K)} (t) - \overline{\widehat{X}}_i^{(K)} \bigg\}} \right]^2}{\mathlarger{\sum}_{t = 1}^T \bigg\{X_i (t) - \overline{X}_i \bigg\}^2 \mathlarger{\sum}_{t = 1}^T \bigg\{ \widehat{X}_i^{(K)} (t) - \overline{\widehat{X}}_i^{(K)} \bigg\} ^2},
$$
where
$$
\overline{X}_i = \frac{1}{N} \sum_{t=1}^T X_i (t) \quad \text{and} \quad \overline{\widehat{X}}_i^{(K)} = \frac{1}{N} \sum_{t=1}^T \widehat{X}_i^{(K)} (t).
$$

\subsection{Predicted Residual Sum of Squares (PRESS)}

The predicted residual sum of squares (PRESS) statistics measures the discrepancy, in terms of total squared error, between the observed data $X_i (\mathbf{t})$ and its predicted value $\widehat{X}^{(K)}_{i} (\mathbf{t})$. 
The PRESS statistic is defined as the squared Euclidean distance between the observed data and its predicted value:
\begin{align*}
    \text{PRESS}\left\{X_i (\mathbf{t}), \widehat{X}^{(K)}_{i} (\mathbf{t}) \right\} 
&= 
\sum_{t = 1}^T \left\{ X_{i}(t) - \widehat{X}^{(K)}_{i} (t)\right\}^2 \\
&=
\bigg\| X_i (\mathbf{t}) - \widehat{X}^{(K)}_{i} (\mathbf{t}) \bigg\|^2\\
&=
\left\{X_i (\mathbf{t}) - \widehat{X}^{(K)}_{i} (\mathbf{t})\right\}^\top \left\{X_i (\mathbf{t}) - \widehat{X}^{(K)}_{i} (\mathbf{t})\right\}.
\end{align*}
While we have defined the PRESS statistic for individual observations, given our focus on individual information loss values, the total PRESS statistic summed over all observations is typically used to summarize the information loss in a PCA representation \parencite{bro_cross-validation_2008}.

\section*{Proof: Relationship between PRESS and $1 -\rho^2$ for PCA-based Projections}

When PCA is employed, $\widehat{X}^{(K)}_{i}(\mathbf{t})$ represents the projection of $X_{i}(\mathbf{t})$ onto \( K \)-dimensional subspace spanned by the first $K$ eigenvectors from PCA. That is
$$
 \widehat{X}^{(K)}_{i} (\mathbf{t}) = \underbrace{\boldsymbol{\Phi}_K}_{\text{Eigenvectors}} \underbrace{\boldsymbol{\Phi}_K^\top X_i(\mathbf{t})}_{\text{PC Scores}}
 = \mathbf{P} X_i(\mathbf{t}).
$$
where \( \mathbf{P} = \bm{\Phi}_K \bm{\Phi}_K^\top \) is the projection matrix, and \( \bm{\Phi}_K \in \mathbb{R}^{p \times K} \) satisfies \( \bm{\Phi}_K^\top \bm{\Phi}_K = \mathbf{I}_K \) (i.e., the eigenvectors, by definition, are orthogonal).
In the case where both $X_i(\mathbf{t})$ and $\widehat{X}^{(K)}_{i}(\mathbf{t})$ are mean-centered, i.e.,
$$
    X_i(\mathbf{t}) = \mathbf{A} X_i(\mathbf{t})
 \quad \textbf{and} \quad 
 \widehat{X}^{(K)}_{i}(\mathbf{t}) = \mathbf{A}\widehat{X}^{(K)}_{i}(\mathbf{t})
$$
for the centering matrix \( \mathbf{A} = \mathbf{I} - \frac{1}{T} \mathbf{1} \mathbf{1}^\top \), we can expand PRESS and $\rho^2$ to get
\begin{align*}
    \text{PRESS}\bigg\{X_{i}(\mathbf{t}),  \widehat{X}^{(K)}_{i} (\mathbf{t})\bigg\} &= \bigg\{X_{i}(\mathbf{t}) -  \widehat{X}^{(K)}_{i} (\mathbf{t})\bigg\}^\top \bigg\{X_{i}(\mathbf{t}) -  \widehat{X}^{(K)}_{i} (\mathbf{t})\bigg\} \\
    &= 
    X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t}) - 2 X_{i}(\mathbf{t})^\top  \widehat{X}^{(K)}_{i} (\mathbf{t}) + \widehat{X}^{(K)\top}_{i\cdot}  \widehat{X}^{(K)}_{i} (\mathbf{t}) \\
    &= X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t}) - 2 X_{i}(\mathbf{t})^\top  \widehat{X}^{(K)}_{i} (\mathbf{t}) + X_{i}(\mathbf{t})^\top \boldsymbol{\Phi}_K \underbrace{\boldsymbol{\Phi}_K^\top \boldsymbol{\Phi}_K}_{= \mathbf{I}_K} \boldsymbol{\Phi}_K^\top X_{i}(\mathbf{t}) \\
    &=  X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t}) - 2 X_{i}(\mathbf{t})^\top  \widehat{X}^{(K)}_{i} (\mathbf{t}) + X_{i}(\mathbf{t})^\top \underbrace{\boldsymbol{\Phi}_K \boldsymbol{\Phi}_K^\top X_{i}(\mathbf{t})}_{= \widehat{X}^{(K)}_{i} (\mathbf{t})} \\
    &= X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t}) - 2 X_{i}(\mathbf{t})^\top  \widehat{X}^{(K)}_{i} (\mathbf{t}) + X_{i}(\mathbf{t})^\top  \widehat{X}^{(K)}_{i} (\mathbf{t}) \\
    &= X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t}) - X_{i}(\mathbf{t})^\top  \widehat{X}^{(K)}_{i} (\mathbf{t}).
\end{align*}
Likewise, we have 
\begin{align*}
    \rho^2  \bigg\{X_{i}(\mathbf{t}),  \widehat{X}^{(K)}_{i} (\mathbf{t})\bigg\}
    =&
    \bigg\{\widehat{X}^{(K)}(\mathbf{t})X_{i}(\mathbf{t}) X_{i}(\mathbf{t})^\top \widehat{X}^{(K)}(\mathbf{t}) \bigg\}
     \bigg\{X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t}) \underbrace{\widehat{X}^{(K)\top}(\mathbf{t}) \widehat{X}^{(K)}_{i} (\mathbf{t})}_{=X_{i}(\mathbf{t})^\top  \widehat{X}^{(K)}_{i} (\mathbf{t})} \bigg\}^{-1} \\
     =& 
     \bigg\{\widehat{X}^{(K)^\top}(\mathbf{t})X_{i}(\mathbf{t}) X_{i}(\mathbf{t})^\top \widehat{X}^{(K)}(\mathbf{t}) \bigg\}
     \bigg\{X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t}) X_{i}(\mathbf{t})^\top  \widehat{X}^{(K)}_{i} (\mathbf{t}) \bigg\}^{-1} \\
     =&
     \widehat{X}^{(K)^\top}(\mathbf{t})X_{i}(\mathbf{t}) \underbrace{X_{i}(\mathbf{t})^\top \widehat{X}^{(K)}(\mathbf{t})
     \bigg\{X_{i}(\mathbf{t})^\top \widehat{X}^{(K)}(\mathbf{t}) \bigg\}^{-1}}_{=\mathbf{I}}
     \bigg\{X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t})\bigg\}^{-1}  \\
     =& \widehat{X}^{(K)^\top}(\mathbf{t})X_{i}(\mathbf{t})  \bigg\{X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t})\bigg\}^{-1} \\
     =&
     \widehat{X}^{(K)}(\mathbf{t})^\top X_{i}(\mathbf{t})  \bigg\{X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t})\bigg\}^{-1} + 
     \bigg\{X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t})\bigg\}  \bigg\{X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t})\bigg\}^{-1} \\ &- 
     \bigg\{X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t})\bigg\}  \bigg\{X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t})\bigg\}^{-1} \\
     =& 
     1 - \underbrace{\bigg\{ X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t}) - \widehat{X}^{(K)}(\mathbf{t})^\top X_{i}(\mathbf{t}) \bigg\}}_{= \text{PRESS}\bigg\{X_{i}(\mathbf{t}),  \widehat{X}^{(K)}_{i} (\mathbf{t})\bigg\}} \bigg\{X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t})\bigg\}^{-1} \\
     =&
     1 - \text{PRESS}\bigg\{X_{i}(\mathbf{t}),  \widehat{X}^{(K)}_{i} (\mathbf{t})\bigg\} \bigg\{X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t})\bigg\}^{-1}.
\end{align*}
Thus, when we use the complement of the squared correlation as our loss, we have
\begin{align*}
    \text{Loss} \left\{ X_i(\mathbf{t}) \right\}  &= 1 - \rho^2  \bigg\{X_{i}(\mathbf{t}),  \widehat{X}^{(K)}_{i} (\mathbf{t})\bigg\} \\
        &= \text{PRESS}\bigg\{X_{i}(\mathbf{t}),  \widehat{X}^{(K)}_{i} (\mathbf{t})\bigg\} \bigg\{X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t})\bigg\}^{-1} \\
    &=
    \frac{\text{PRESS}\bigg\{X_{i}(\mathbf{t}),  \widehat{X}^{(K)}_{i} (\mathbf{t})\bigg\}}{\sum_{t=1}^T X_i(t)^2} = \frac{\text{PRESS}\bigg\{X_{i}(\mathbf{t}),  \widehat{X}^{(K)}_{i} (\mathbf{t})\bigg\}}{\| X_i(\mathbf{t})\|^2},
\end{align*}
which is the PRESS statistic, normalized by the squared Euclidean norm of the vector $X_i(\mathbf{t})$.
While an analogous relationship does not hold exactly when $X_i(t)$ and $\widehat{X}_i(\mathbf{t})$ are not centered, because of the non-commutativity between the centering matrix $\mathbf{A}$ and the projection matrix $\mathbf{P}$, it provides us with an intuition for $1-\rho^2$ as a measure of distance between the observed data and its predictions, that is normalized to account for the scale of the data.

