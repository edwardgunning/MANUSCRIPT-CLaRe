\section{Additional Details on Squared Correlation Loss} \label{sec:squared-correlation}


\subsection{Predicted Residual Sum of Squares (PRESS)}

The predicted residual sum of squares (PRESS) statistics measures the discrepancy, in terms of total squared error, between the observed data $X_i (\mathbf{t})$ and its predicted value $\widehat{X}^{(K)}_{i} (\mathbf{t})$. 
The PRESS statistic is defined as the squared Euclidean distance between the observed data and its predicted value:
\begin{align*}
    \text{PRESS}\left\{X_i (\mathbf{t}), \widehat{X}^{(K)}_{i} (\mathbf{t}) \right\} 
&= 
\sum_{t = 1}^T \left\{ X_{i}(t) - \widehat{X}^{(K)}_{i} (t)\right\}^2 \\
&=
\bigg\| X_i (\mathbf{t}) - \widehat{X}^{(K)}_{i} (\mathbf{t}) \bigg\|^2\\
&=
\left\{X_i (\mathbf{t}) - \widehat{X}^{(K)}_{i} (\mathbf{t})\right\}^\top \left\{X_i (\mathbf{t}) - \widehat{X}^{(K)}_{i} (\mathbf{t})\right\}.
\end{align*}

\subsection{Complement of the Predicted Correlation Squared ($1-\rho^2$)}

Our loss measure is the complement of the squared correlation correlation among the observed data $X_i (\mathbf{t})$ and its predicted value $\widehat{X}^{(K)}_{i} (\mathbf{t})$:
$$
1- \rho^2 \left\{X_i (\mathbf{t}), \widehat{X}^{(K)}_{i} (\mathbf{t}) \right\} =
1 - \frac{\left[ \mathlarger{\sum}_{t = 1}^T{\bigg\{X_i (t) - \overline{X}_i \bigg\} \bigg\{ \widehat{X}_i^{(K)} (t) - \overline{\widehat{X}}_i^{(K)} \bigg\}} \right]^2}{\mathlarger{\sum}_{t = 1}^T \bigg\{X_i (t) - \overline{X}_i \bigg\}^2 \mathlarger{\sum}_{t = 1}^T \bigg\{ \widehat{X}_i^{(K)} (t) - \overline{\widehat{X}}_i^{(K)} \bigg\} ^2},
$$
where
$$
\overline{X}_i = \frac{1}{N} \sum_{t=1}^T X_i (t) \quad \text{and} \quad \overline{\widehat{X}}_i^{(K)} = \frac{1}{N} \sum_{t=1}^T \widehat{X}_i^{(K)} (t).
$$

\subsection{Relationship Between PRESS and $1-\rho^2$ for PCA}

\subsection*{Definitions and Setup}

When PCA is employed, $\widehat{X}^{(K)}_{i}(\mathbf{t})$ represents projection of $X_{i}(\mathbf{t})$ onto \( K \)-dimensional subspace spanned by the first $K$ eigenvectors from PCA. That is
$$
 \widehat{X}^{(K)}_{i} (\mathbf{t}) = \underbrace{\boldsymbol{\Phi}_K}_{\text{Eigenvectors}} \underbrace{\boldsymbol{\Phi}_K^\top X_i(\mathbf{t})}_{\text{PC Scores}}
 = \mathbf{P} X_i(\mathbf{t}).
$$
where \( \mathbf{P} = \bm{\Phi}_K \bm{\Phi}_K^\top \) is the projection matrix, and \( \bm{\Phi}_K \in \mathbb{R}^{p \times K} \) satisfies \( \bm{\Phi}_K^\top \bm{\Phi}_K = \mathbf{I}_K \) (i.e., the eigenvectors, by definition, are orthogonal).
Next, define the centered versions of $X_i(\mathbf{t})$ and $\widehat{X}^{(K)}_{i} (\mathbf{t})$:
\[
X_{i,c}(\mathbf{t}) = \mathbf{A} X_{i}(\mathbf{t}), \quad 
\widehat{X}^{(K)}_{i,c}(\mathbf{t}) = \mathbf{A} \widehat{X}^{(K)}_{i}(\mathbf{t}),
\]
where \( \mathbf{A} = \mathbf{I}_T - \frac{1}{T} \mathbf{1} \mathbf{1}^\top \) is the centering matrix.
This also leads to the relation
$$
\widehat{X}_{i,c}^{(K)} (\mathbf{t}) = \mathbf{P} X_{i,c}(\mathbf{t}),
$$
because 
$$
\widehat{X}_{i,c}^{(K)} = \mathbf{A} \mathbf{P}X_i(\mathbf{t}).
$$
Now 
$$
AP = (I - 
$$


% 1. Let \( X_{i}(\mathbf{t}) \in \mathbb{R}^p \) denote the original observation, and let \( \widehat{X}^{(K)}_{i}(\mathbf{t}) \) be its projection onto a \( K \)-dimensional subspace:
% \[
% \widehat{X}^{(K)}_{i}(\mathbf{t}) = \mathbf{P} X_{i}(\mathbf{t}),
% \]
% where \( \mathbf{P} = \bm{\Phi} \bm{\Phi}^\top \) is the projection matrix, and \( \bm{\Phi} \in \mathbb{R}^{p \times K} \) satisfies \( \bm{\Phi}^\top \bm{\Phi} = \mathbf{I}_K \).

2. Define the centered versions:
\[
X_{i,c}(\mathbf{t}) = \mathbf{A} X_{i}(\mathbf{t}), \quad 
\widehat{X}^{(K)}_{i,c}(\mathbf{t}) = \mathbf{A} \widehat{X}^{(K)}_{i}(\mathbf{t}),
\]
where \( \mathbf{A} = \mathbf{I}_p - \frac{1}{p} \mathbf{1} \mathbf{1}^\top \) is the centering matrix.

3. The squared Pearson correlation is defined as:
\[
\rho^2 \bigg\{ X_{i}(\mathbf{t}), \widehat{X}^{(K)}_{i} (\mathbf{t}) \bigg\} 
= \frac{\bigg( X_{i,c}(\mathbf{t})^\top \widehat{X}^{(K)}_{i,c}(\mathbf{t}) \bigg)^2}
{\bigg\| X_{i,c}(\mathbf{t}) \bigg\|^2 \bigg\| \widehat{X}^{(K)}_{i,c}(\mathbf{t}) \bigg\|^2}.
\]

4. The squared centered Euclidean distance is:
\[
\text{PRESS}\bigg\{ X_{i}(\mathbf{t}), \widehat{X}^{(K)}_{i} (\mathbf{t}) \bigg\} 
= \bigg\| X_{i,c}(\mathbf{t}) - \widehat{X}^{(K)}_{i,c}(\mathbf{t}) \bigg\|^2.
\]

---

\subsection*{Derivation}

Substitute \( \widehat{X}^{(K)}_{i,c}(\mathbf{t}) = \mathbf{P} X_{i,c}(\mathbf{t}) \) into the expression for PRESS:
\[
\text{PRESS}\bigg\{ X_{i}(\mathbf{t}), \widehat{X}^{(K)}_{i} (\mathbf{t}) \bigg\}
= \bigg\| X_{i,c}(\mathbf{t}) - \mathbf{P} X_{i,c}(\mathbf{t}) \bigg\|^2.
\]
Expand the norm:
\[
\text{PRESS}\bigg\{ X_{i}(\mathbf{t}), \widehat{X}^{(K)}_{i} (\mathbf{t}) \bigg\}
= \bigg\| X_{i,c}(\mathbf{t}) \bigg\|^2 - 2 X_{i,c}(\mathbf{t})^\top \mathbf{P} X_{i,c}(\mathbf{t}) + \bigg\| \mathbf{P} X_{i,c}(\mathbf{t}) \bigg\|^2.
\]
Using the idempotence of \( \mathbf{P} \), \( \big\| \mathbf{P} X_{i,c}(\mathbf{t}) \big\|^2 = X_{i,c}(\mathbf{t})^\top \mathbf{P} X_{i,c}(\mathbf{t}) \), so:
\[
\text{PRESS}\bigg\{ X_{i}(\mathbf{t}), \widehat{X}^{(K)}_{i} (\mathbf{t}) \bigg\}
= \bigg\| X_{i,c}(\mathbf{t}) \bigg\|^2 - X_{i,c}(\mathbf{t})^\top \mathbf{P} X_{i,c}(\mathbf{t}).
\]

From the definition of \( \rho^2 \):
\[
X_{i,c}(\mathbf{t})^\top \mathbf{P} X_{i,c}(\mathbf{t}) 
= \rho^2 \bigg\{ X_{i}(\mathbf{t}), \widehat{X}^{(K)}_{i} (\mathbf{t}) \bigg\} \bigg\| X_{i,c}(\mathbf{t}) \bigg\|^2.
\]

Substitute this into the expression for PRESS:
\[
\text{PRESS}\bigg\{ X_{i}(\mathbf{t}), \widehat{X}^{(K)}_{i} (\mathbf{t}) \bigg\}
= \bigg\| X_{i,c}(\mathbf{t}) \bigg\|^2 \bigg( 1 - \rho^2 \bigg\{ X_{i}(\mathbf{t}), \widehat{X}^{(K)}_{i} (\mathbf{t}) \bigg\} \bigg).
\]

---

\subsection*{Final Relationship}

Rearranging:
\[
1 - \rho^2 \bigg\{ X_{i}(\mathbf{t}), \widehat{X}^{(K)}_{i} (\mathbf{t}) \bigg\}
= \frac{\text{PRESS}\bigg\{ X_{i}(\mathbf{t}), \widehat{X}^{(K)}_{i} (\mathbf{t}) \bigg\}}
{\bigg\| X_{i,c}(\mathbf{t}) \bigg\|^2}.
\]

---

% The predicted correlation denoted $\rho^2$ measures the correlation among the observed data $X_i (\mathbf{t})$ and its predicted value $\widehat{X}^{(K)}_{i} (\mathbf{t})$. It is defined as
% $$
% \rho^2 \left\{X_i (\mathbf{t}), \widehat{X}^{(K)}_{i} (\mathbf{t}) \right\}
% =
% \frac{\left\{ \mathlarger{\sum}_{t = 1}^T{X_i (t) \widehat{X}_i^{(K)} (t)} \right\}^2}{\mathlarger{\sum}_{t = 1}^T \bigg\{ X_i (t) \bigg\} ^2 \mathlarger{\sum}_{t = 1}^T  \bigg\{ \widehat{X}_i^{(K)} (t)\bigg\}^2}.
% $$

\subsection{Relationship Between PRESS and $\rho^2$ for PCA}





\subsubsection{For Individual Observations}

We expand PRESS and $\rho^2$ to better express the relationship between them, when PCA is employed as the latent feature representation method. 
We start by recalling that, when PCA is applied to the mean-centered matrix $\mathbf{X}$, the $K$-dimensional prediction is given by 
$$
 \widehat{X}^{(K)}_{i} (\mathbf{t}) = \underbrace{\boldsymbol{\Phi}_K}_{\text{PCs}} \underbrace{\boldsymbol{\Phi}_K^\top X_i(\mathbf{t})}_{\text{PC Scores}}
$$
\begin{align*}
    \text{PRESS}\bigg\{X_{i}(\mathbf{t}),  \widehat{X}^{(K)}_{i} (\mathbf{t})\bigg\} &= \bigg\{X_{i}(\mathbf{t}) -  \widehat{X}^{(K)}_{i} (\mathbf{t})\bigg\}^\top \bigg\{X_{i}(\mathbf{t}) -  \widehat{X}^{(K)}_{i} (\mathbf{t})\bigg\} \\
    &= 
    X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t}) - 2 X_{i}(\mathbf{t})^\top  \widehat{X}^{(K)}_{i} (\mathbf{t}) + \widehat{X}^{(K)\top}_{i\cdot}  \widehat{X}^{(K)}_{i} (\mathbf{t}) \\
    &= X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t}) - 2 X_{i}(\mathbf{t})^\top  \widehat{X}^{(K)}_{i} (\mathbf{t}) + X_{i}(\mathbf{t})^\top \boldsymbol{\Phi}_K \underbrace{\boldsymbol{\Phi}_K^\top \boldsymbol{\Phi}_K}_{= \mathbf{I}_K} \boldsymbol{\Phi}_K^\top X_{i}(\mathbf{t}) \\
    &=  X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t}) - 2 X_{i}(\mathbf{t})^\top  \widehat{X}^{(K)}_{i} (\mathbf{t}) + X_{i}(\mathbf{t})^\top \underbrace{\boldsymbol{\Phi}_K \boldsymbol{\Phi}_K^\top X_{i}(\mathbf{t})}_{= \widehat{X}^{(K)}_{i} (\mathbf{t})} \\
    &= X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t}) - 2 X_{i}(\mathbf{t})^\top  \widehat{X}^{(K)}_{i} (\mathbf{t}) + X_{i}(\mathbf{t})^\top  \widehat{X}^{(K)}_{i} (\mathbf{t}) \\
    &= X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t}) - X_{i}(\mathbf{t})^\top  \widehat{X}^{(K)}_{i} (\mathbf{t}).
\end{align*}
Likewise, we have 
\begin{align*}
    \rho^2  \bigg\{X_{i}(\mathbf{t}),  \widehat{X}^{(K)}_{i} (\mathbf{t})\bigg\} 
    &=
    \bigg\{\widehat{X}^{(K)^\top}_{i\cdot}X_{i}(\mathbf{t}) X_{i}(\mathbf{t})^\top \widehat{X}^{(K)^\top}_{i\cdot} \bigg\}
     \bigg\{X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t}) \underbrace{\widehat{X}^{(K)\top}_{i\cdot} \widehat{X}^{(K)}_{i} (\mathbf{t})}_{=X_{i}(\mathbf{t})^\top  \widehat{X}^{(K)}_{i} (\mathbf{t})} \bigg\}^{-1} \\
     &= 
     \bigg\{\widehat{X}^{(K)^\top}_{i\cdot}X_{i}(\mathbf{t}) X_{i}(\mathbf{t})^\top \widehat{X}^{(K)^\top}_{i\cdot} \bigg\}
     \bigg\{X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t}) X_{i}(\mathbf{t})^\top  \widehat{X}^{(K)}_{i} (\mathbf{t}) \bigg\}^{-1} \\
     &=
     \widehat{X}^{(K)^\top}_{i\cdot}X_{i}(\mathbf{t}) \underbrace{X_{i}(\mathbf{t})^\top \widehat{X}^{(K)^\top}_{i\cdot}
     \bigg\{X_{i}(\mathbf{t})^\top \widehat{X}^{(K)^\top}_{i\cdot} \bigg\}^{-1}}_{=1}
     \bigg\{X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t})\bigg\}^{-1}  \\
     &= \widehat{X}^{(K)^\top}_{i\cdot}X_{i}(\mathbf{t})  \bigg\{X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t})\bigg\}^{-1} \\
     &=
     \widehat{X}^{(K)^\top}_{i\cdot}X_{i}(\mathbf{t})  \bigg\{X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t})\bigg\}^{-1} + 
     \bigg\{X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t})\bigg\}  \bigg\{X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t})\bigg\}^{-1} -
     \bigg\{X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t})\bigg\}  \bigg\{X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t})\bigg\}^{-1} \\
     &= 
     1 - \underbrace{\bigg\{ X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t}) - \widehat{X}^{(K)^\top}_{i\cdot}X_{i}(\mathbf{t}) \bigg\}}_{= \text{PRESS}\bigg\{X_{i}(\mathbf{t}),  \widehat{X}^{(K)}_{i} (\mathbf{t})\bigg\}} \bigg\{X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t})\bigg\}^{-1} \\
     &=
     1 - \text{PRESS}\bigg\{X_{i}(\mathbf{t}),  \widehat{X}^{(K)}_{i} (\mathbf{t})\bigg\} \bigg\{X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t})\bigg\}^{-1}.
\end{align*}
Thus, when we use the complement of the squared correlation as our loss, we have
\begin{align*}
    \text{Loss} \left\{ X_i(\mathbf{t}) \right\}  &= 1 - \rho^2  \bigg\{X_{i}(\mathbf{t}),  \widehat{X}^{(K)}_{i} (\mathbf{t})\bigg\} \\
        &= \text{PRESS}\bigg\{X_{i}(\mathbf{t}),  \widehat{X}^{(K)}_{i} (\mathbf{t})\bigg\} \bigg\{X_{i}(\mathbf{t})^\top X_{i}(\mathbf{t})\bigg\}^{-1} \\
    &=
    \frac{\text{PRESS}\bigg\{X_{i}(\mathbf{t}),  \widehat{X}^{(K)}_{i} (\mathbf{t})\bigg\}}{\sum_{t=1}^T X_i(t)^2},
\end{align*}
which is the PRESS statistic, normalized by the magnitude of the vector $X_i(\mathbf{t})$.

\subsubsection{For the Total Dataset}

When calculate the \emph{total} statistics over all observations, we have an analogous relations:
\begin{align*}
     \text{PRESS}\left(\mathbf{X}, \widehat{\mathbf{X}}^{(K)}\right) &= \sum_{i=1}^N \left(X_{i\cdot} - \widehat{X}^{(K)}_{i\cdot}\right)^\top \left(X_{i\cdot} - \widehat{X}^{(K)}_{i\cdot}\right) \\
     &= \sum_{i=1}^N X_{i\cdot}^\top X_{i\cdot} - X_{i\cdot}^\top \widehat{X}^{(K)}_{i\cdot}.
\end{align*}
\begin{align*}
    \rho^2\left(\mathbf{X}, \widehat{\mathbf{X}}^{(K)}\right)
    &= \frac{\left(\sum_{i=1}^N \sum_{t=1}^T X_{it} \widehat{X}_{it}^{(K)}\right)^2}{
    \left(\sum_{i=1}^N \sum_{t=1}^T X_{it} X_{it}\right)
    \left(\sum_{i=1}^N \sum_{t=1}^T \widehat{X}_{it}^{(K)} \widehat{X}_{it}^{(K)}\right)
    } \\
    &= 
    \frac{\left(\sum_{i=1}^N \widehat{X}^{(K)^\top}_{i\cdot}X_{i\cdot} \right)^2}{
    \left(\sum_{i=1}^N \sum_{t=1}^T X_{it} X_{it}\right)
    \left(\sum_{i=1}^N \sum_{t=1}^T \widehat{X}_{it}^{(K)} \widehat{X}_{it}^{(K)}\right)
    } \\
\end{align*}

When PRESS is calculated \emph{in sample}, we have
When calculate the \emph{total} statistics over all observations, we have an analogous relations:
\begin{align*}
     \text{PRESS}\left(\mathbf{X}, \widehat{\mathbf{X}}^{(K)}\right) &= \sum_{i=1}^N \left(X_{i\cdot} - \widehat{X}^{(K)}_{i\cdot}\right)^\top \left(X_{i\cdot} - \widehat{X}^{(K)}_{i\cdot}\right) \\
     &= \sum_{i=1}^N X_{i\cdot}^\top X_{i\cdot} - X_{i\cdot}^\top \widehat{X}^{(K)}_{i\cdot} \\
     &= \text{need to show } \sum_{r=1}^R \widehat{\lambda}_r  - \sum_{r=K+1}^R \widehat{\lambda}_r  \\
     &= \sum_{r=1}^K \lambda_r
\end{align*}



\subsection*{Definitions and Setup}

1. Let \( X_{i}(\mathbf{t}) \in \mathbb{R}^p \) denote the original observation, and let \( \widehat{X}^{(K)}_{i}(\mathbf{t}) \) be its projection onto a \( K \)-dimensional subspace:
\[
\widehat{X}^{(K)}_{i}(\mathbf{t}) = \mathbf{P} X_{i}(\mathbf{t}),
\]
where \( \mathbf{P} = \bm{\Phi} \bm{\Phi}^\top \) is the projection matrix, and \( \bm{\Phi} \in \mathbb{R}^{p \times K} \) satisfies \( \bm{\Phi}^\top \bm{\Phi} = \mathbf{I}_K \).

2. Define the centered versions:
\[
X_{i,c}(\mathbf{t}) = \mathbf{A} X_{i}(\mathbf{t}), \quad 
\widehat{X}^{(K)}_{i,c}(\mathbf{t}) = \mathbf{A} \widehat{X}^{(K)}_{i}(\mathbf{t}),
\]
where \( \mathbf{A} = \mathbf{I}_p - \frac{1}{p} \mathbf{1} \mathbf{1}^\top \) is the centering matrix.

3. The squared Pearson correlation is defined as:
\[
\rho^2 \bigg\{ X_{i}(\mathbf{t}), \widehat{X}^{(K)}_{i} (\mathbf{t}) \bigg\} 
= \frac{\bigg( X_{i,c}(\mathbf{t})^\top \widehat{X}^{(K)}_{i,c}(\mathbf{t}) \bigg)^2}
{\bigg\| X_{i,c}(\mathbf{t}) \bigg\|^2 \bigg\| \widehat{X}^{(K)}_{i,c}(\mathbf{t}) \bigg\|^2}.
\]

4. The squared centered Euclidean distance is:
\[
\text{PRESS}\bigg\{ X_{i}(\mathbf{t}), \widehat{X}^{(K)}_{i} (\mathbf{t}) \bigg\} 
= \bigg\| X_{i,c}(\mathbf{t}) - \widehat{X}^{(K)}_{i,c}(\mathbf{t}) \bigg\|^2.
\]

---

\subsection*{Derivation}

Substitute \( \widehat{X}^{(K)}_{i,c}(\mathbf{t}) = \mathbf{P} X_{i,c}(\mathbf{t}) \) into the expression for PRESS:
\[
\text{PRESS}\bigg\{ X_{i}(\mathbf{t}), \widehat{X}^{(K)}_{i} (\mathbf{t}) \bigg\}
= \bigg\| X_{i,c}(\mathbf{t}) - \mathbf{P} X_{i,c}(\mathbf{t}) \bigg\|^2.
\]
Expand the norm:
\[
\text{PRESS}\bigg\{ X_{i}(\mathbf{t}), \widehat{X}^{(K)}_{i} (\mathbf{t}) \bigg\}
= \bigg\| X_{i,c}(\mathbf{t}) \bigg\|^2 - 2 X_{i,c}(\mathbf{t})^\top \mathbf{P} X_{i,c}(\mathbf{t}) + \bigg\| \mathbf{P} X_{i,c}(\mathbf{t}) \bigg\|^2.
\]
Using the idempotence of \( \mathbf{P} \), \( \big\| \mathbf{P} X_{i,c}(\mathbf{t}) \big\|^2 = X_{i,c}(\mathbf{t})^\top \mathbf{P} X_{i,c}(\mathbf{t}) \), so:
\[
\text{PRESS}\bigg\{ X_{i}(\mathbf{t}), \widehat{X}^{(K)}_{i} (\mathbf{t}) \bigg\}
= \bigg\| X_{i,c}(\mathbf{t}) \bigg\|^2 - X_{i,c}(\mathbf{t})^\top \mathbf{P} X_{i,c}(\mathbf{t}).
\]

From the definition of \( \rho^2 \):
\[
X_{i,c}(\mathbf{t})^\top \mathbf{P} X_{i,c}(\mathbf{t}) 
= \rho^2 \bigg\{ X_{i}(\mathbf{t}), \widehat{X}^{(K)}_{i} (\mathbf{t}) \bigg\} \bigg\| X_{i,c}(\mathbf{t}) \bigg\|^2.
\]

Substitute this into the expression for PRESS:
\[
\text{PRESS}\bigg\{ X_{i}(\mathbf{t}), \widehat{X}^{(K)}_{i} (\mathbf{t}) \bigg\}
= \bigg\| X_{i,c}(\mathbf{t}) \bigg\|^2 \bigg( 1 - \rho^2 \bigg\{ X_{i}(\mathbf{t}), \widehat{X}^{(K)}_{i} (\mathbf{t}) \bigg\} \bigg).
\]

---

\subsection*{Final Relationship}

Rearranging:
\[
1 - \rho^2 \bigg\{ X_{i}(\mathbf{t}), \widehat{X}^{(K)}_{i} (\mathbf{t}) \bigg\}
= \frac{\text{PRESS}\bigg\{ X_{i}(\mathbf{t}), \widehat{X}^{(K)}_{i} (\mathbf{t}) \bigg\}}
{\bigg\| X_{i,c}(\mathbf{t}) \bigg\|^2}.
\]

---