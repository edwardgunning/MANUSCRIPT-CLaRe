\section{Software}\label{sec:software}

The main function in the \pkg{GLaRe} software package is \texttt{GLaRe()}, which performs and summarises the cross-validated information of a given latent feature method on a given dataset.
A sample call to the \texttt{GLaRe()} function is as follows:
\begin{lstlisting}[language=R]
mnist_pca <- GLaRe(mat = x_train,
                   latent_dim_from = 1,
                   latent_dim_to = 400,
                   latent_dim_by = 20,
                   learn = "pca",
                   verbose = TRUE)
\end{lstlisting}
The \texttt{GLaRe()} function computes individual, cross-validated values of a information loss for a latent feature representation method defined by \texttt{learn}, on dataset stored in the matrix \texttt{mat}, across a range of latent feature dimensions defined by a grid of equally-spaced values from \texttt{latent\_dim\_from} to \texttt{latent\_dim\_by} in increments of \texttt{latent\_dim\_by}.

\subsection{Learning Functions}\label{sec:learning-functions}

The learning function is the main engine of \texttt{GlaRe()}.
The learning function function takes as arguments a data matrix $\mathbf{X}$ and latent feature dimension $K$, and learns the encoding and decoding transformation functions $f_K$ and $f_K^{-1}$.
For example, the learning function in PCA simply computes the first $K$ eigenvectors of the empirical covariance matrix of $\mathbf{X}$, and $f_K$ and $f_K^{-1}$ comprise the matrix multiplication by the first $K$ eigenvectors.
In contrast, for an autoencoder, $f_K$ and $f_K^{-1}$ are general functions that map to and from a $K$-dimensional space and are parametrised by flexible neural networks.
In \texttt{GLaRe()}, the learning function defined by the \texttt{learn} argument. We provide three built-in learning functions that can be used in the software, as well as the option to specify a user-defined bespoke function:
\begin{enumerate}
    \item Setting \texttt{learn = "pca"} specifies a PCA representation. The eigenvectors are computed by the Singular Value Decomposition (SVD) algorithm. In cases where $N < T$ (e.g., the Glaucoma or Gels data) the latent feature dimension for PCA can be, at most, $N-1$. Hence the maximum latent feature dimension \texttt{latent\_dim\_to} is set to a default of $\min(N-1, T)$ when using PCA.
    \item Setting \texttt{learn = "dwt"} (or \texttt{learn = "dwt.2d"} for data on a 2-dimensional domain) specifies a thresholded wavelet representation. For encoding, the DWT is applied to $\mathbf{X}$ and the most important $K$ latent features (i.e., wavelet coefficients) are learned from the data and are retained. The decoding function then applies the inverse DWT to the retained features. Our implementation uses the \pkg{wavselim} \proglang{R} package \parencite{whitcher_waveslim_2024} which uses the Daubechies orthonormal compactly supported wavelet of length $8$ \parencite{daubechies_ten_1992}, least asymmetric family and uses periodic boundary conditions. Additional details are described in Appendix \ref{sec:wavelet-thresholding-algorithm}.
    \item Setting \texttt{learn = "ae"} specifies an autoencoder representation. We implement the autoencoder using the \pkg{keras} \proglang{R} package \parencite{kalinowski_keras_2024}. The encoder and decoder functions are parametrised by neural networks with two hidden layers (defaulting to sizes of $600$ and $200$, respectively) and rectified linear unit (ReLu) activation functions.
    A linear activation is used to map from the second hidden layer of the encoder to the latent space, and a choice between linear and sigmoid (default) activation functions can be used to map from the second hidden layer of the decoder back to the latent space.
    By default, the autoencoder is trained for $100$ epochs using the ADAM stochastic gradient descent algorithm \parencite{kingma_adam_2017} to minimise either the mean squared error (default) or binary cross-entropy loss functions using a mini-batch size of $16$.
    \item Setting \texttt{learn = "user"} allows the user to specify their own latent feature representation method. With this setting, the user must supply the learning function for their method, that takes the data matrix $\mathbf{X}$ and the latent feature dimension $\mathbf{K}$ as inputs and returns a list with two elements: functions named \texttt{Encode} and \texttt{Decode} implementing the the learned encoding and decoding transformation functions $f_K$ and $f_K^{-1}$.
\end{enumerate}

\subsection{Squared Correlation Loss}\label{sec:loss-functions}

In principle, any loss function can be used with \texttt{GLaRe()} and the package has been structured such that different loss functions can be used in future iterations.
The current implementation uses the compliment of the squared correlation
$$
1- \rho^2 \left(X_i (\mathbf{t}), \widehat{X}^{(K)}_{i} (\mathbf{t}) \right) =
1 - \frac{\left( \mathlarger{\sum}_{t = 1}^T{\bigg(X_i (t) - \overline{X}_i \bigg) \bigg( \widehat{X}_i^{(K)} (t) - \overline{\widehat{X}}_i^{(K)} \bigg)} \right)^2}{\mathlarger{\sum}_{t = 1}^T \bigg(X_i (t) - \overline{X}_i \bigg)^2 \mathlarger{\sum}_{t = 1}^T \bigg( \widehat{X}_i^{(K)} (t) - \overline{\widehat{X}}_i^{(K)} \bigg) ^2},
$$
where
$$
\overline{X}_i = \frac{1}{N} \sum_{t=1}^T X_i (t) \quad \text{and} \quad \overline{\widehat{X}}_i^{(K)} = \frac{1}{N} \sum_{t=1}^T \widehat{X}_i^{(K)} (t).
$$
The criterion $1- \rho^2$ is a relative measure, bounded between $0$ and $1$: $1- \rho^2 = 0$ indicates losslessness and $1- \rho^2 = 1$ indicates that no information is preserved by the latent feature representation method.
In Appendix \ref{sec:squared-correlation}, we highlight connections between squared correlation, the Predicted Residual Sum of Squares (PRESS) loss and the scree plot for PCA.

\subsection{Cross-Validation Algorithm}

Given the learning function (Section \ref{sec:learning-functions}), the loss function (Section \ref{sec:loss-functions}), the data matrix $\mathbf{X}$ and a grid of values for $K$, \texttt{GLaRe()} implements a $k$-fold cross validation algorithm to estimate the individual generalisation errors. 
A high-level overview of the internal structure of \texttt{GLaRe()} is provided in Algorithm \ref{alg:glare}.
In principle, it is possible to employ leave-one-out cross validation by setting the number of folds to the number of observations $N$.
However, in most of our real-world applications we employ a non-exhaustive search (e.g., $5$ or $10$-fold cross-validation) due to computational considerations.
When a non-exhaustive search is used, variability is induced when splitting of the data into folds (i.e., the data rows are randomly shuffled and then divided into folds), so we recommend fixing the random seed before running \texttt{GLaRe()} so that results are reproducible.
It can also be useful to re-run \texttt{GLaRe()} using different seeds to assess the sensitivity of results to the split and decide whether to employ a more computationally intensive exhaustive search.



\begin{algorithm}
\caption{Internal structure of the \texttt{GLaRe()} function.}\label{alg:glare}
\KwData{
\begin{itemize}
    \item \underline{Data matrix}: $\mathbf{X}$.
    \item \underline{Learning function}: \texttt{learn($x$, k)}.
    \item \underline{Loss function}: \texttt{loss($x$, $\hat{x}$)}.
    \item \underline{Latent dimensions sequence}: $K_{from}$, $K_{to}$, $K_{by}$ with length $K_{len}$.
    \item \underline{Number of folds for cross-validation}: \texttt{num\_fold}
\end{itemize}}
\KwResult{
\begin{itemize}
    \item \underline{Training Loss}: \texttt{Training\_Loss} ($N \times K_{len}$ matrix)
    \item \underline{Validation Loss}: \texttt{Validation\_Loss} ($N \times K_{len}$ matrix)
\end{itemize}
}

\For{$K$ from $K_{from}$ to $K_{to}$ by $K_{by}$ \Comment*[r]{Training}}{
    \texttt{full\_model} \gets \texttt{learn($x$ = $\mathbf{X}$, k = $K$)};
    
    $\mathbf{X}^*$ \gets \texttt{full\_model}.\texttt{Encode($\mathbf{X}$)};

    $\widehat{\mathbf{X}}$ \gets \texttt{full\_model}.\texttt{Decode($\mathbf{X}^*$)};
    
    \For{i from $1$ to $N$}{
        \texttt{Training\_Loss[$i$, $K$]} \gets \texttt{loss}($x = X_{i}, \hat{x} = \widehat{X}_{i}$);
        }
    }


    Divide $i=1,\dots,N$ randomly into \texttt{num\_fold} folds \Comment*[r]{Cross-Validation}
    
\For{$j$ from $1$ to \texttt{num\_fold}} {

    $\mathbf{X}_{\text{Training}} \gets \mathbf{X}[i \text{ not in fold }j,]$;

    $\mathbf{X}_{\text{Validation}} \gets \mathbf{X}[i \text{ in fold }j,]$;

    \For{$K$ from $K_{from}$ to $K_{to}$ by $K_{by}$}{
        
        \texttt{cv\_model} \gets \texttt{learn($x$ = $\mathbf{X}_{\text{Training}}$, k = $K$)};

        $\mathbf{X}^*_{\text{Validation}}$ \gets \texttt{cv\_model}.\texttt{Encode($\mathbf{X}_{\text{Validation}}$)};

        $\widehat{\mathbf{X}}$_{\text{Validation}} \gets \texttt{cv\_model}.\texttt{Decode($\mathbf{X}^*_{\text{Validation}}$)};
        
        \For{$i$ in fold $j$}{
            \texttt{Validation\_Loss[$i$, $K$]} \gets \texttt{loss}($x$ = $X_{i}$, $\hat{x}$ = $\widehat{X}_{\text{Validation}, i}$);
            }
        }
    }  
    Summarise and Return \texttt{Training\_Loss} and \texttt{Validation\_Loss};
}
\end{algorithm}

\subsection{Software Outputs}

The \texttt{GLaRe()} function computes and returns the cross-validated information loss for each individual observation in the dataset.
The distribution of these independent values is summarized by the main plot that is returned by default by the \texttt{GLaRe()} function (Figure {\color{red}X}).
The overall, or average, cross-validated loss is displayed in yellow, with the analogous loss computed on the training data shown in green for comparison.
Then, different quantiles of the distribution of individual cross-validated are displayed to summarise the full distribution: the minimum and maximum are shown in blue and red, respectively, a user-specified quantile of the distribution (set by the \texttt{cvqlines} argument) is displayed in purple and the quantile of the distribution being used as the cut-off criterion $\alpha$ (defaulting to 0.95, i.e., the $95$th percentile) is displayed in light grey.
The corresponding value of the tolerance level $\epsilon$ is overlaid as grey dashed horizontal line, and hence the latent feature dimension (i.e., location on the $x$-axis) at which the two grey lines meet corresponds to the qualifying criterion.
The \texttt{GLaRe()} function also returns a heatmap as an alternative summary of the individual cross-validated loss distribution, and additional wrapper functions in the package produce a dotplot of the full distribution, a function that plots the ratio of training to cross-validated losses and functions to display the reconstructions of individual observations. Further details of additional functionalities are provided in Appendix \ref{sec:additional-outputs}.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/glare-anatomy-plot.pdf}
    \caption{The summary plot produced by \texttt{GlaRe()}, demonstrated on the Glaucoma dataset with a PCA representation.}
    \label{fig:glare-anatomy-plot}
\end{figure}