\section{Discussion}\label{sec:discussion}

We have introduced CoLLaRe, an evaluation framework for assessing and comparing latent feature representation methods with a focus on compactness and (near-)losslessness.
A distinguishing feature of CoLLaRe is its focus on the full distribution of generalization errors rather than aggregated metrics such as average or total information loss.
Our framework uses cross-validation to estimate the full distribution of generalization errors and proposes to choose a representation such that a tolerance level of generalization error is met for generalized quantiles of this distribution (e.g., worst case or $95$th percentile) while ensuring that the representation is as compact as possible.
Thus, CoLLaRe enables the selection of a compact, near-lossless latent feature representations that ensures statistical modeling in the latent space can accurately reflect the underlying mechanisms of the true data-generating process.


% Unlike traditional approaches that rely on single summary metrics that are aggregated over all observations, GLaRe provides a more nuanced evaluation, capturing variability across individual observations.
% To characterize this distribution, we introduced key concepts such as the tolerance level, cut-off criterion, and qualifying dimension, which enable tailored assessments of latent feature representation methods.
% We have provided an accompanying software package GLaRe which implements the framework and provides interpretable graphical summaries.


One of CoLLaRe's strengths is that it facilitates comparisons between methods, allowing comparisons between traditional tools such as PCA and modern approaches such as autoencoders. 
Through case studies on three datasetsâ€”Glaucoma, Proteomic Gels, and MNIST, we have demonstrated how CoLLaRe can guide the selection of the most suitable latent feature representation based on dataset characteristics.
The results from these case studies reinforce the importance of such context-specific evaluation, as the preferred representation method varied across datasets. 
For instance, the MNIST dataset, with its large sample size relative to feature dimension, benefited from the flexibility of the non-linear AE representation. 
In contrast, for the Proteomic Gels dataset, which is characterized by a small sample size relative to its high-dimensional features, the fixed DWT representation was preferred to the more flexible PCA and AE representations.
We performed an experiment by manually decreasing the sample size of the Glaucoma dataset and comparing the PCA and DWT representations which further highlighted the trade-off between flexible methods and sample size.
These case studies emphasize the role of dataset characteristics, such as sample size, dimensionality and variance structure, in determining the most appropriate latent feature representation. 
CoLLaRe is a valuable framework to compare methods under a consistent set of criteria in such contexts.

We have also described and documented our accompanying \proglang{R} package, GLaRe, which implements the CoLLaRe framework and provides intuitive graphical summaries for the user.
GLaRe provides a flexible implementation of the framework where the user can specify the criteria (e.g., tolerance level, attainment rate).
It provides built-in implementations of three latent feature representation methods -- PCA, DWT and autoencoder -- but it also allows the user to easily specify specify a latent feature representation method of their own..
It is publicly available and can be employed by practitioners in any analysis that relies on latent feature representation methods.


% GLaRe places a unique focus on estimating the full distribution of generalization error for a given dataset, which is more sensitive and informative than summary or total measures.
% We have coined new terminology, e.g., ``tolerance level", ``cut-off criterion", ``qualifying dimension" and ``qualifying criterion", to characterize this distribution and use it to assess latent feature representation methods.
% Our software produces a summary plot as well as several other visualizations to summarize and characterize this distribution.

% A key feature of GLaRe is that it is not tied to any latent feature representation method and can be used to compare among several methods, as demonstrated through our case studies.
% The results of these case studies emphasize the utility of GLaRe, as each of the motivating datasets favored a different latent feature representation method.
% They also re-enforce that sample size plays (alongside data structure) an important role when choosing a latent feature representation method.
% For the MNIST dataset, the sample size ($N=60000$) is large relative to the feature dimension ($T=784$) so it was possible to estimate a flexible non-linear transformation of the data using the AE that provided an optimal representation of the data.
% On the other hand, the Proteomic Gels data has a small sample size ($N=53$) relative to the feature dimension ($T=556206$) so the fixed DWT representation was preferred to the more flexible PCA and AE representations.
% We performed an experiment by manually decreasing the sample size of the Glaucoma dataset and comparing the PCA and DWT representations which further highlighted the trade-off between flexible methods and sample size.
% These case studies underscore the critical role of dataset characteristics, such as sample size and dimensionality, in determining the most appropriate latent feature representation. 
% CoLLaRe's flexibility in comparing methods under consistent criteria is particularly valuable in such contexts.


Some limitations and future directions of this work are as follows.
Our framework focuses on compactness and near-losslessness, which are two of the most important properties of a latent feature representation.
However, other properties might also need to be considered when selecting among representations, e.g., distribution of features in the latent space, interpretability of the latent features, computational time, and effort.
The current framework should also be extended to handle dependent (e.g., multilevel, longitudinal, temporal/ spatial) data by including structured variants of cross-validation for dependent datasets
\parencite{bergmeir_note_2018, hornung_evaluating_2023, roberts_cross-validation_2017}.
In our case studies, we used standard versions of PCA, DWT and AE to facilitate general and straightforward comparisons but future work could consider specialized implementations (e.g., smoothed functional PCA or convolutional autoencoders). 
Finally, from a software development perspective, the addition of a Graphical User Interface (GUI) using \proglang{Shiny} \parencite{chang_shiny_2021}, parallelization of the cross-validation algorithm and the addition of new wrapper functions is of interest.

% {\color{purple}FIX THIS!}

% Firstly, we focused on providing standard, rather than specialized, implementations of PCA and the AE.
% While it is possible to use the \texttt{learn = "user"} setting to specify user-defined transformations, e.g., smoothed functional PCA for curve data or convolutional AEs for image data, a future direction is to build these methods in along the standard PCA, DWT and AE options.
% Likewise, we favored the squared correlation loss due to its tractability and inherent links to conventional loss quantities in classical PCA (Appendix \ref{sec:squared-correlation}), but our software is structured such that future iterations can use alternative other loss functions \parencite[e.g., concordance index, see][]{yang_quantile_2020} that are specified or provided by the user.
% It would also be useful to include structured variants of cross-validation for dependent datasets (e.g., hierarchical, longitudinal or spatial/ temporal dependence structures) \parencite{bergmeir_note_2018, hornung_evaluating_2023, roberts_cross-validation_2017}.
% From a software development perspective, the addition of a Graphical User Interface (GUI) using \proglang{Shiny} \parencite{chang_shiny_2021}, parallelization of the cross-validation algorithm and the addition of new wrapper functions is of interest.