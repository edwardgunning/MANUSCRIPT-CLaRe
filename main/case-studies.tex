\section{Case Studies}\label{sec:case-studies}

In this section, we introduce three motivating datasets -- the Glaucoma dataset, the Proteomic Gels dataset and the MNIST digits dataset (Figure \ref{fig:combined-data-objects} (a)--(c)).
For each dataset, we apply our CLaRe framework to select among the following three latent feature representation methods\footnote{Specific implementation details for the methods are described in Section \ref{sec:software}.}:
\begin{enumerate}
    \item \textbf{Principal Components Analysis (PCA)} \parencite{hotelling_analysis_1933} is the most prevalent dimension reduction technique in statistics, used to represent multivariate data in a lower-dimensional space that preserves as much variance as possible. 
    Using terminology from Section \ref{sec:materials-and-methods}, the encoding transform in PCA is the linear projection of the data matrix $\mathbf{X}$ onto the matrix containing the leading $K$ eigenvectors of the empirical covariance matrix, typically denoted by $\mathbf{U}$, which yields the latent features, known as PCA scores.
    The decoding transformation is the inverse linear mapping of the latent features back to the data space, which consists of multiplying the latent features by the transposed matrix of eigenvectors.
    \item \textbf{Thresholded Discrete Wavelet Transform (DWT)}: The DWT \parencite{daubechies_wavelet_1990, daubechies_ten_1992} is a dimension reduction technique that linearly transforms a discretely-sampled signal of length $T$ into a set of $T$ latent features, known as wavelet coefficients, that are localized in both time and frequency. In theory, both the DWT and the inverse DWT (IDWT) are linear transformations but they benefit from efficient recursive algorithms in practice  \parencite{daubechies_ten_1992}.
    
    To use the DWT as a dimension reduction technique, we exploit its inherent sparsity property, i.e., that a signal can typically be well represented by $K \ll T$ wavelet coefficients. We apply the DWT to each observation and then learn the $K$ most important wavelet coefficients jointly across all observations (see Appendix \ref{sec:wavelet-thresholding-algorithm} for more details) -- these coefficients are retained as the $K$ latent features. 
    The remaining latent features are set to $0$ when applying the IDWT as the decoding transform. 
    Thus, encoding consists of the composition of two transformations: (1) the linear DWT transformation and (2) the nonlinear selection of the $K$ most important coefficients while zeroing the rest.
    The thresholded DWT has been used in functional data analysis \parencite[see, e.g.,][]{morris_automated_2011} and for simplicity we abbreviate as DWT hereafter.

    \item \textbf{An Autoencoder (AE)} \parencite{rumelhart_learning_1986} is a dimension reduction technique from the field of machine learning.
    The encoding and decoding transformations that map the data to the latent representation space, known as the \emph{bottleneck}, are general non-linear functions parameterized by neural networks
    $$
    f_K(X_i) = \text{NN}_{\boldsymbol{\theta}_E}(X_i (\mathbf{t})) \text{ and } g_K\big\{\left(X_{i1}^*, \dots,  X_{iK}^* \right)^\top\big\} = \text{NN}_{\boldsymbol{\theta}_D} \big\{\left(X_{i1}^*, \dots,  X_{iK}^* \right)^\top\big\},
    $$
    where $\boldsymbol{\theta}_E$ and $\boldsymbol{\theta}_D$ are vectors of weights for the encoding and decoding networks with dimensions determined by the respective neural network architectures.
    These weights are learned to minimize the reconstruction error
    $$
    \sum_{i=1}^N \bigg\| X_i (\mathbf{t}) - \widehat{X}_i^{(K)}(\mathbf{t}) \bigg\|^2,
    $$
    or another suitable loss measure for the problem at hand \parencite[e.g., binary cross-entropy for binary data, see][]{kalinowski_keras_2024}.
\end{enumerate}
The three methods originate from different fields -- PCA is a conventional statistical method, the DWT originates from signal processing and autoencoders are ubiquitous in machine learning and artificial intelligence.
They also exhibit differences in the amount of structure they learn from the data.
On one hand, the thresholded DWT learns the least structure from the data -- the linear DWT and IDWT transforms are fixed and only the ordering of the coefficients to threshold is learned from the data.
PCA is more flexible and learns more structure from the data because the matrix of eigenvectors $\mathbf{U}$ that defines the encoding and decoding transformations computed from the empirical covariance matrix of the data.
An AE is even more flexible and learns greater structure from the data, because the linearity restriction in PCA is relaxed and the AE learns general non-linear encoding and decoding transformations.

To explore the practical implications of these differences and how they relate to sample size, we conclude in Section \ref{sec:sample-size-experiment} by presenting the results of an experiment where we artificially decimate the sample size of the Glaucoma dataset.
We demonstrate that more flexible empirical methods (e.g., PCA) work well when there is enough data to reliably estimate the latent features, and that fixed methods (e.g., DWT) perform better when there is not enough data.
For all applications in this section, we use a tolerance level of $\epsilon = 0.05$, an attainment rate of $1 - \alpha=0.95$ and employ the complement of the squared correlation as our loss function.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/data-plot.pdf}
    \caption{
    A sample observation from each of our three motivating datasets.
    \textbf{(a)}: A sample glaucoma image, representing a polar azimuthal projection of MPS functions for a single eye at one IOP level. A cubed-root transformation is applied for visualization.
    \textbf{(b)}: A sample 2D gel electrophoresis image, showing proteomic content in the brain tissue of a rat.
    \textbf{(c)}: A sample MNIST digit image, which is a $28 \times 28$ pixel greyscale image of a single handwritten digit.}
    \label{fig:combined-data-objects}
\end{figure}



\subsection{Glaucoma Data}

Glaucoma is considered a leading factor in blindness. It is characterized by damage to the optic nerve, which can be induced by \emph{intraocular pressure (IOP)}. 
To investigate proposed hypotheses about the relationship between glaucoma and IOP, \textcite{fazio_age-related_2014} developed instrumentation to measure the mechanical strain on the scleral surface of the eye at different levels of IOP. 
The measurements were summarized for each location at different locations on the scleral surface of the eye as \emph{maximum principal strain} (MPS). MPS was computed on 34 eyes from 19 normal human donors. It was measured in the posterior globe of both eyes on a partial spherical domain with $120$ circumferential locations $\upsilon \in (0^{\circ}, 360^{\circ})$ and $120$ meridional locations $\theta \in (9^{\circ}, 24^{\circ})$.
These measurements were taken at 9 different IOP levels.
One study goal for this dataset was to test the hypothesis that scleral strain decreases with age thereby leaving the optic nerve head susceptible to damage which could be a contributing factor in the development of glaucoma \parencite{lee_bayesian_2019}.
Figure \ref{fig:combined-data-objects} (a) displays a two-dimensional polar azimuthal projection of a single observation from the Glaucoma data.

We let $X_i(t)$ denote the MPS function for a single eye at a specific IOP level so that $i = 1, \dots, N = 306$ ($34$ eyes at $9$ IOP levels).
The data lives on a domain $\mathcal{T}$ which is the portion of the sphere defined by $(\upsilon, \theta)$ for $\upsilon \in (0^{\circ}, 360^{\circ})$ and $\theta \in (9^{\circ}, 24^{\circ})$.
Therefore, each observation $X_i(t)$ is recorded a common grid of size $T = 14400$.
The recordings are indexed by the $14400$-dimensional vector $\mathbf{t} = \boldsymbol{\upsilon} \times \boldsymbol{\theta}$, where $\boldsymbol{\upsilon}$ represents $120$ equally-spaced measurements of $\upsilon$ along $(0^{\circ}, 360^{\circ})$ and $\boldsymbol{\theta}$ represents $120$ equally-spaced measurements of $\theta$ along $(9^{\circ}, 24^{\circ})$.
We denote the vector of measurements for the $i$th observation as $X_i(\mathbf{t})$, so that the full dataset can be represented by the $N \times T$ data matrix $\mathbf{X}$, containing $X_1(\mathbf{t}), \dots, X_N(\mathbf{t})$ in its rows.

Figure \ref{fig:eye-results} displays the summary plot from the application of \texttt{GLaRe()} to the Glaucoma data.
PCA is the most suitable latent feature representation method for this dataset because it achieves the qualifying criterion with qualifying dimension $qd=41$, whereas DWT and AE do not achieve the qualifying criterion for $K \leq 301$.
A grid of equally-spaced values from $1$ to $301$ in increments of $10$ was used for the latent feature dimensions.
Although it was possible to use larger latent feature dimensions for the DWT and AE, it was deemed unnecessary because the qualifying criterion was achieved for PCA with a qualifying dimension $qd=51$ and is the favored method for this dataset.
PCA achieved a compression ratio of $351:1$ ($T = 14400$ to $qd = 41$).
The computation times for PCA, DWT and AE were $1.6$, $1.7$ and $67.4$ minutes, respectively.


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/eye-results.pdf}
    \caption{Summary \texttt{GLaRe()} plot for the Glaucoma data. A grid of equally-spaced values from $1$ to $301$ in increments of $10$ was used for the latent feature dimensions. 
    % \textbf{Note}: For PCA, the maximum number of non-zero eigenvalues was $\leq361$ so only results for $K \leq 361$ are displayed on the summary plot.
    The legend for plots (b) and (c) has been suppressed so that it does not mask the worst-case observation and because it is identical to the legend for plot (a).}
    \label{fig:eye-results}
\end{figure}

\subsection{Proteomic Gels Data}

In neurobiology, a particularly important issue is the identification of changes responsible for the transition from non-dependent drug use to addiction which is characterized by drug intake behavior. Studies on rats have shown that rats given a 6-12 hours/day access to cocaine or heroin have significant increase in drug intake while rats given 1 hour/day access kept the same level of intake over time.
The corresponding neurochemical changes in the extended part of the brain amygdala relate to cellular effects that affect protein expression and function which can be detected via proteomic analysis. To study this phenomenon, experiments were conducted in which the rats were trained to get cocaine by pressing a lever: 6 rats were given 1hour/day access, 7 rats were given 6 hours /day access and 8 rats were used for control with no access to cocaine. The rats were euthanized after some time, and their brains studied \parencite{morris_pinnacle_2008}. 
Two-dimensional gel electrophoresis was used to study the proteomic content in the brain tissues. 
Between two and three gels were obtained from each rat and brain region, resulting in a dataset of 53 gel images from 21 rats.
Each gel image has $556,206$ pixel intensities observed on a $646 \times 861$ grid. 
A research goal for this dataset was to study the proteins which are differentially expressed in the brains of rats that were exposed to cocaine for a long time versus those that were not.
This can be done by finding regions in the gel images where image intensity is significantly different across groups \parencite{morris_statistical_2012}. 
Figure \ref{fig:combined-data-objects} (b) displays a single gel image observation.

We denote a single observation from the Proteomic Gels dataset as $X_i(t)$, where $i=1, \dots, N = 53$.
Here, $t$ represents a location $(t_1, t_2)$ in the two-dimensional Euclidean domain defined by the Cartesian product $\mathcal{T} = [0, 1] \times [0, 1]$.
Measurements of each observation are made at the vector of locations $\mathbf{t} = \mathbf{t}_1 \times \mathbf{t}_2$ where $\mathbf{t}_1$ and $\mathbf{t}_2$ represent vectors of $646$ and $861$ equally-spaced points along $[0, 1]$, respectively.
Then $X_i (\mathbf{t})$ denotes the $T = 556206 ( = 646 \times 861)$-dimensional vector containing the measurements of the $i$th observation at the locations in $\mathbf{t}$, and the $N \times T$ data matrix $\mathbf{X}$ contains $X_1 (\mathbf{t}), \dots, X_N (\mathbf{t})$ in its rows.

Figure \ref{fig:gels-results} displays the summary plot from the application of \texttt{GLaRe()} to the Proteomic Gels data.
Because this dataset only contains $N=53$ observations, the maximum possible latent dimension for PCA is $52$ and PCA does not achieve the qualifying criterion for $K \leq 52$.
Due to computational constraints, we ran the AE for the same range of candidate latent feature dimensions, and it did not achieve the qualifying criterion.
Performance did not appear to improve as the latent feature dimension was increased.
In contrast, the maximum latent dimension for the DWT is not constrained and hence we ran \texttt{GLaRe()} on a grid of equally-spaced values from $1$ to $8000$ in increments of $100$.
The qualifying dimension for the DWT is $qd=7801$.
Hence, the DWT is the favored representation method for the Proteomic Gels dataset and it provides a compression ratio of $71:1$ ($T = 556206$ to $qd = 7801$).
The computation times for PCA, DWT and AE were $0.9$, $47.6$ and $109.6$ minutes, respectively.


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/gels-results.pdf}
    \caption{Summary \texttt{GLaRe()} plot for the Proteomic Gels data. For PCA and AE, a grid of equally-spaced values from $1$ to $53$ in increments of $10$ was used for the latent feature dimensions.
    For the DWT,  a grid of equally-spaced values from $1$ to $8000$ in increments of $10$ was used for the latent feature dimensions.}
    \label{fig:gels-results}
\end{figure}


\subsection{MNIST Digits Data}

The MNIST (Modified National Institute of Standards and Technology) database of handwritten digits was compiled by \textcite{lecun_mnist_1998}, from a larger collection of images from the National Institute of Standards and Technology (NIST).
It comprises a training set of $60000$ images and a test set of $10000$ images, representing hand-written digits from $0$ to $9$ (i.e., $10$ distinct digits/classes).
The original black and white images from NIST were modified into $28 \times 28$ pixel greyscale images.
The MNIST dataset has been employed extensively in computer vision and deep learning applications as a test case for image reconstruction and digit identification/ classification models.
The dataset can be represented in an $N \times T$ matrix $\mathbf{X}$, where $N = 60000$ (in the case of the training set) and $T= 784$ $(= 28 \times 28)$.
We let $X_i(t)$ represent the value of the $i$th greyscale image at pixel location $t$, where $t \in \mathbf{t} = \{1, \dots, 28\} \times \{1, \dots, 28\}$.
Then the $i$th row of the data matrix $\mathbf{X}$ contains the $784$-dimensional vector $X_i(\mathbf{t})$, i.e., measurements of the $i$th observation at the vector of pixel locations in $\mathbf{t}$.
We normalise the greyscale images so that $X_i(t) \in [0, 1]$ for all $t \in \mathbf{t}$ and $i = 1, \dots, N$. 
Figure \ref{fig:combined-data-objects} (c) displays a single digit from the MNIST dataset.
The full dataset (training and test) is publicly available in the \pkg{keras} \proglang{R} package \parencite{kalinowski_keras_2024} and can be loaded using the \texttt{dataset\_mnist()} function.

Figure \ref{fig:mnist-results} displays the summary plot from the application of \texttt{GLaRe()} to the MNIST data.
A grid of equally-spaced values from $1$ to $381$ in increments of $20$ was used for the latent feature dimensions.
All three latent feature representation methods achieve the qualifying criterion within this range: $qd = 201$ for PCA, $qd = 321$ for the DWT and $qd = 101$ for the AE.
Hence, the AE is the preferred method for this dataset because it provides the most compact (i.e., smallest $qd$) representation.
The AE has a compression ratio of $8:1$ ($T = 784$ to $K = 101$).
The computation times for PCA, DWT and AE were $16.2$, $27.4$ and $1115.3$ minutes, respectively.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/mnist-results.pdf}
    \caption{Summary \texttt{GLaRe()} plot for the MNIST data. A grid of equally-spaced values from $1$ to $381$ in increments of $20$ was used for the latent feature dimensions.}
    \label{fig:mnist-results}
\end{figure}


\subsection{Sample Size Experiment}\label{sec:sample-size-experiment}

We  present the results of an experiment that demonstrates the dependence of flexible latent feature representation methods such as PCA and AE on sample size.
In PCA, the encoding and decoding transformations are learned entirely from the data, so it is highly dependent on having a sufficient sample size.
In contrast, the DWT transformation is fixed a-priori and only the ordering of the wavelet coefficients to retain is learned from the data and hence there is less reliance on sample size.
We demonstrate this concept empirically on the Glaucoma dataset. 
We start with the full dataset ($N=306$) and then sub-sample the dataset to create smaller datasets of sizes $N=153$, $N=76$ and $N=38$ respectively.
We run \texttt{GLaRe()} to compare the performance of PCA and the thresholded DWT as the sample size is successively degraded.
In all cases, because of the small sample sizes, we use leave-one-out rather than $k$-fold cross-validation.

Figure \ref{fig:eye-sample-size-results-results-01} displays the results of the experiment.
The PCA results are displayed in the top row and the DWT results are displayed in the bottom row.
As PCA can only estimate, at most, $\min(N-1, T)$ features and in this case $N<T$, we see that the maximum possible number of latent features changes as the sample size decreases.
In contrast, there is no restriction on the number of latent features for the DWT and we manually choose a maximum of $K=800$, which is sufficient to achieve the qualifying criterion (with $\epsilon=0.05$ and $1 - \alpha=0.95$) in all four cases.
The greater reliance of PCA on sample size is reflected in Figure \ref{fig:eye-sample-size-results-results-01} in several ways.
Firstly, the displayed quantiles of the cross-validated loss distribution, in particular the maximum, $0.95$ and $0.9$ quantiles (red, grey and purple lines) increase noticeably and systematically for PCA as the sample size is decreased, but they remain more stable for the DWT.
The dependence is also reflected by the separation between the training and validation mean losses (green vs. yellow lines) as sample size is decreased to $N=38$ for PCA but not the DWT, which demonstrates that PCA is unable to estimate a generalizable representation when the sample size is small, even if the training loss is satisfactory.
Finally, we can achieve the qualifying criterion using the DWT in all four cases (albeit with a large number of features), but we only achieve it for PCA with sample sizes $N=306$ and $N=153$.
The experiment is subject to sampling variability induced in the sub-sampling stage, so it is repeated using different random seeds in Appendix \ref{sec:additional-results}.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/eye-sample-size-results-results-real.pdf}
    \caption{Results of the experiment to assess the effect of sample size on different latent feature representations. The Glaucoma data was used to create smaller datasets of size $N=153$, $N=76$ and $N=38$. \texttt{GLaRe()} was used to compare the representations provided by PCA (first row) and DWT (second row) as the sample size was decreased. Leave-one-out cross-validation was used in all cases.}
    \label{fig:eye-sample-size-results-results-01}
\end{figure}